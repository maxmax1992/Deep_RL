{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replications from paper \"Asyncronous Actor Critic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "# from skimage.transform import resize\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "import collections\n",
    "import gym\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous one-step Q-learning\n",
    "<img src=\"q-learning-pseudocode.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(10, requires_grad=True)\n",
    "b = a.detach().clone()\n",
    "a\n",
    "b[1] = 3\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 10 mean 8.0\n",
      "finished ep  0\n",
      "0.9955\n",
      "last 10 mean 10.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1d9dc0878b69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-1d9dc0878b69>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDQN_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mstate0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-1d9dc0878b69>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gymenv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gymenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# Simple policy network (for selecting the action)\n",
    "class Q_network(nn.Module):\n",
    "    def __init__(self, observation_space, n_actions, lr=0.0001):\n",
    "        super(Q_network, self).__init__()\n",
    "        self.onehotmask = torch.zeros(observation_space).to(device)\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(observation_space, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, n_actions)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    #recieves 1 integer, one hot encode it to \n",
    "    def forward(self, state):\n",
    "        state = torch.tensor(state).float()\n",
    "        x = self.seq(state)\n",
    "        return x\n",
    "    \n",
    "    def train(self, inputs, targets):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.smooth_l1_loss(inputs, targets)\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "    def set_weights(self, other_nn):\n",
    "        self.load_state_dict(other_nn.state_dict())\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "# GLOBAL SHARED FOR EACH THREAD\n",
    "DQN = Q_network(4, env.action_space.n, 0.0025)\n",
    "DQN_target = Q_network(4, env.action_space.n, 0.0025)\n",
    "DQN.set_weights(DQN_target)\n",
    "T = 0\n",
    "\n",
    "class Replay_Memory():\n",
    "    def __init__(self, maxlen=5000):\n",
    "        self.array = deque([], maxlen=maxlen)\n",
    "    \n",
    "    def sample(self, n_batch):\n",
    "        results = random.sample(self.array, min(len(self.array), n_batch))\n",
    "        return results\n",
    "    \n",
    "    def append(self, value):\n",
    "        self.array.append(value)\n",
    "    \n",
    "def process_batch(batch, dqn, dqn_target):\n",
    "    outputs_ = []\n",
    "    targets_ = []\n",
    "    \n",
    "    for (outputs, state0, action, reward, state, done) in batch:\n",
    "        outputs_.append(outputs)\n",
    "        y = reward\n",
    "        temp = outputs.detach().clone()\n",
    "        if not done:\n",
    "            out = state.reshape((1, 4))\n",
    "            dqn_target_term = torch.max(dqn_target(out)[0]).item()\n",
    "            y += 0.99 * dqn_target_term\n",
    "        temp[0, action] = y\n",
    "        targets_.append(temp)\n",
    "    return torch.stack(outputs_).to(device), torch.stack(targets_).to(device)\n",
    "        \n",
    "\n",
    "    \n",
    "       \n",
    "def train():\n",
    "    start = 1.00\n",
    "    end = 0.10\n",
    "    n_step_decays = 3000\n",
    "    gamma  = 0.99\n",
    "    g_t = 0\n",
    "    target_update_I = 50\n",
    "    T_max = 10000\n",
    "    N_episodes = 1500\n",
    "    debug = False\n",
    "    \n",
    "    env._max_episode_steps = T_max\n",
    "    replay_memory = Replay_Memory(1000)\n",
    "    total_rewards = []\n",
    "    running_reward = deque([0], maxlen=10)\n",
    "    for ep in range(N_episodes):\n",
    "\n",
    "        done = False\n",
    "        state0 = env.reset()\n",
    "        t = 0\n",
    "        total_r = 0\n",
    "        while True:\n",
    "#             env.render()\n",
    "            curr_epsilon = max(end, start - (start-end)*g_t/n_step_decays)\n",
    "\n",
    "            if np.random.uniform() < curr_epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                DQN_output = DQN(state0.reshape((1, 4)))\n",
    "            else:\n",
    "                DQN_output = DQN(state0.reshape((1, 4)))\n",
    "                action = torch.argmax(DQN_output[0]).item()\n",
    "\n",
    "            state, reward, done, info = env.step(action)\n",
    "            reward = 1\n",
    "            total_r += reward\n",
    "            replay_memory.append([DQN_output, state0, action, reward, state, done])\n",
    "            \n",
    "            if g_t % 30 == 0:\n",
    "                batch = replay_memory.sample(30)\n",
    "                outputs, targets = process_batch(batch, DQN, DQN_target)\n",
    "                DQN.train(outputs, targets)\n",
    "                \n",
    "            state0 = state\n",
    "            # update target DQN\n",
    "            if g_t % target_update_I == 0:\n",
    "                DQN_target.set_weights(DQN)\n",
    "            g_t += 1\n",
    "            t += 1\n",
    "            if done:\n",
    "#                 print('ep ', ep, ' done')\n",
    "#                 if ep % 10 == 0:\n",
    "                running_reward.append(total_r)\n",
    "                print('last 10 mean', np.mean(running_reward))\n",
    "                total_rewards.append(total_r)\n",
    "                break\n",
    "#         print(curr_epsilon)\n",
    "        if ep % 250 == 0:\n",
    "            print('finished ep ', ep)\n",
    "            print(curr_epsilon)\n",
    "    plt.plot(np.arange(len(total_rewards)), total_rewards)\n",
    "    plt.show()\n",
    "    env.close()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01543734, -0.01796325,  0.00392776,  0.03123437])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.zeros(10)\n",
    "d = c.clone()\n",
    "d[0] = 1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker Starting\n",
      "my_service Starting\n",
      "worker Exiting\n",
      "my_service Exiting\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "def worker():\n",
    "    print(threading.currentThread().getName(), 'Starting')\n",
    "    time.sleep(2)\n",
    "    print(threading.currentThread().getName(), 'Exiting')\n",
    "\n",
    "def my_service():\n",
    "    print(threading.currentThread().getName(), 'Starting')\n",
    "    time.sleep(3)\n",
    "    print(threading.currentThread().getName(), 'Exiting')\n",
    "\n",
    "t = threading.Thread(name='my_service', target=my_service)\n",
    "w = threading.Thread(name='worker', target=worker)\n",
    "# w2 = threading.Thread(target=worker) # use default name\n",
    "\n",
    "w.start()\n",
    "# w2.start()\n",
    "t.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gymenv)",
   "language": "python",
   "name": "gymenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
