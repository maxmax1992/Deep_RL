{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9738, 0.1549, 0.7427],\n",
      "        [0.3109, 0.3026, 0.2183],\n",
      "        [0.7190, 0.0869, 0.9434],\n",
      "        [0.5827, 0.6285, 0.4375],\n",
      "        [0.0023, 0.5938, 0.1499]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[-0.6534, -0.2158, -0.6944],\n",
      "        [-2.0448, -0.7748, -0.1905],\n",
      "        [ 0.4455,  0.6352,  2.0486],\n",
      "        [ 0.5485, -0.2452, -0.4419],\n",
      "        [-1.0188,  0.1026,  0.3128]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "print(x.dtype)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([[-0.6534, -0.2158, -0.6944],\n",
      "        [-2.0448, -0.7748, -0.1905],\n",
      "        [ 0.4455,  0.6352,  2.0486],\n",
      "        [ 0.5485, -0.2452, -0.4419],\n",
      "        [-1.0188,  0.1026,  0.3128]])\n",
      "\n",
      "\n",
      "tensor([[-0.1572, -0.1515, -0.0567],\n",
      "        [-1.3043, -0.4949,  0.1734],\n",
      "        [ 0.9092,  0.7685,  2.2040],\n",
      "        [ 0.6632,  0.2583,  0.0094],\n",
      "        [-0.8203,  0.3181,  1.2291]])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x)\n",
    "print('\\n')\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1572, -0.1515, -0.0567],\n",
      "        [-1.3043, -0.4949,  0.1734],\n",
      "        [ 0.9092,  0.7685,  2.2040],\n",
      "        [ 0.6632,  0.2583,  0.0094],\n",
      "        [-0.8203,  0.3181,  1.2291]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1572, -0.1515, -0.0567],\n",
      "        [-1.3043, -0.4949,  0.1734],\n",
      "        [ 0.9092,  0.7685,  2.2040],\n",
      "        [ 0.6632,  0.2583,  0.0094],\n",
      "        [-0.8203,  0.3181,  1.2291]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "tensor([-0.4582])\n",
      "-0.45823556184768677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.45823556184768677"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())\n",
    "\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "x[0].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "a.add_(1)\n",
    "print(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5418], device='cuda:0')\n",
      "tensor([0.5418], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic gradient derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "y  = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward object at 0x7f67a59fb128>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward>)\n",
      "tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y*3 \n",
    "out = z.mean()\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradients\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.ones(2, 2, requires_grad=True)\n",
    "d = c.sum()\n",
    "d.backward()\n",
    "print(c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-681.1754, -755.3866,    1.7198], grad_fn=<MulBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1, 1, requires_grad=True)\n",
    "c = x.add(3)\n",
    "y = c**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [2.0494, 1.9833, 1.0673, 1.3994, 2.4251],\n",
      "        [4.0988, 3.9666, 2.1346, 2.7987, 4.8503],\n",
      "        [6.1483, 5.9499, 3.2020, 4.1981, 7.2754],\n",
      "        [8.1977, 7.9332, 4.2693, 5.5974, 9.7005]])\n"
     ]
    }
   ],
   "source": [
    "in_  = torch.tensor(np.arange(5, dtype='float32'))\n",
    "W1 = torch.rand(5, 5, requires_grad=True)\n",
    "\n",
    "layer2 = in_.matmul(W1)\n",
    "W2 = torch.rand(5, 5, requires_grad=True)\n",
    "\n",
    "layer3 = layer2.matmul(W2)\n",
    "W3 = torch.rand(5, requires_grad=True)\n",
    "\n",
    "d = layer3.dot(W3)\n",
    "loss = d.sum()\n",
    "# print(loss.backward())\n",
    "loss.backward()\n",
    "print(W1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple full layer neural network Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adam, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None, score=1):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad.add_(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "                \n",
    "                p.data.addcdiv_(step_size*score, exp_avg, denom)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, learning_rate):\n",
    "        super(PolicyNetwork, self).__init__()        \n",
    "        \n",
    "        drop = 0.5\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=input_dim),\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(128, 10),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.optimizer = Adam(self.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.size())\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "#     def __init__(self, input_dim, learning_rate):\n",
    "#         super(PolicyNetwork, self).__init__()        \n",
    "#         self.h1 = nn.Linear(input_dim, 20)\n",
    "#         self.h2 = nn.Linear(20, 10)\n",
    "#         self.out = nn.Linear(10, 1)\n",
    "        \n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "#         self.loss_fn = torch.nn.MSELoss()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = F.leaky_relu(self.h1(x))\n",
    "#         x = F.leaky_relu(self.h2(x))\n",
    "#         x = F.leaky_relu(self.out(x))\n",
    "#         return x\n",
    "    \n",
    "    def train(self, X, y, n_steps, batch_size=64):\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        batch_size=min(batch_size, N)\n",
    "        \n",
    "        for t in range(n_steps):\n",
    "            \n",
    "            batch_indices = np.random.randint(0, N, batch_size)\n",
    "            \n",
    "            y_pred = self.forward(X[batch_indices])\n",
    "            \n",
    "\n",
    "            # Compute and print loss.\n",
    "            loss = self.loss_fn(y_pred, y[batch_indices])\n",
    "#             print(loss.item())\n",
    "\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the variables it will update (which are the learnable\n",
    "            # weights of the model). This is because by default, gradients are\n",
    "            # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "            # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model\n",
    "            # parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its\n",
    "            # parameters\n",
    "            self.optimizer.step()\n",
    "\n",
    "net = PolicyNetwork(4, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "tensor([[0.4908],\n",
      "        [0.4908],\n",
      "        [0.4908],\n",
      "        [0.4908]], grad_fn=<SigmoidBackward>)\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([4, 4])\n",
      "tensor([[0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array(\n",
    "    [[1,2,3,4],\n",
    "    [1,2,3,4],\n",
    "    [1,2,3,4],\n",
    "    [1,2,3,4]], dtype='float64'\n",
    ")\n",
    "targets = np.array([\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1\n",
    "], dtype='float64')\n",
    "print(net(torch.from_numpy(inputs).float()))\n",
    "net.train(torch.from_numpy(inputs).float(), torch.from_numpy(targets).float().view(-1, 1), n_steps=1000, batch_size=2)\n",
    "print(net(torch.from_numpy(inputs).float()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1, 2],[3, 4]])\n",
    "ar2 = np.array([[1, 2],[3, 4]])\n",
    "\n",
    "tensor = torch.from_numpy(arr).float()\n",
    "tensor2 = torch.from_numpy(ar2).float()\n",
    "# print(tensor.size())\n",
    "# batchNorm = nn.LayerNorm(4)\n",
    "# batchNorm(tensor)\n",
    "torch.cat([tensor, tensor2], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], grad_fn=<LeakyReluBackward0>)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.from_numpy(inputs).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-424-bbab0f0c7bda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/capsenv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/capsenv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "print(net.h1.bias.grad)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.h1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capsenv",
   "language": "python",
   "name": "capsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
