{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "from baselines import deepq\n",
    "from baselines.common.atari_wrappers import make_atari\n",
    "from Deep_QNet import QNetwork\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MaxAndSkipEnv' object has no attribute 'frameskip'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b016ab3c4e84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_atari\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PongNoFrameskip-v4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pong-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(env2.frameskip)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaxAndSkipEnv' object has no attribute 'frameskip'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "env = make_atari('PongNoFrameskip-v4')\n",
    "env2 = gym.make('Pong-v0')\n",
    "print(env.frameskip)\n",
    "# print(env2.frameskip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# wrap atari to nicer preprocessed environment\n",
    "env = make_atari('BreakoutNoFrameskip-v4')\n",
    "env = deepq.wrap_atari_dqn(env)\n",
    "\n",
    "# learn on batch of transitions\n",
    "def processBatch(data_arr, df, DQN, DQN_target):\n",
    "    X, Y = [], []\n",
    "    for elem in data_arr:\n",
    "        s1, action, reward, done, s2 = elem.getValues()\n",
    "        y = DQN.predict(s1)\n",
    "        y[action] = reward\n",
    "        if not done:\n",
    "            y[action] = reward + df * max(DQN_target.predict(s2))\n",
    "        X.append(s1)\n",
    "        Y.append(y)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, s1, action, reward, doness, s2):\n",
    "        self.values = s1, action, reward, done, s2\n",
    "\n",
    "    def getValues(self):\n",
    "        return self.values\n",
    "\n",
    "e_start = float(1.00)\n",
    "e_end = float(0.10)\n",
    "decay_frames = 1000000\n",
    "change = float(e_start - e_end) / float(1000000)\n",
    "epsilon = e_start\n",
    "\n",
    "df = 0.99\n",
    "rewards = []\n",
    "\n",
    "DQN = QNetwork(lr = 0.5)\n",
    "DQN_target = DQN.copyModel()\n",
    "\n",
    "replay_memory = deque([], maxlen=1000000)\n",
    "\n",
    "frame = 0\n",
    "learnStep = 0\n",
    "showedFirst = False\n",
    "last100_ep_rewards = deque([], maxlen=100)\n",
    "\n",
    "ep = 0\n",
    "randPolicy = True\n",
    "for i in range(0, 5000000):\n",
    "\n",
    "    frame += 1\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    totalReward = 0\n",
    "    ep += 1\n",
    "\n",
    "\n",
    "    if randPolicy and frame > 50000:\n",
    "        randPolicy = False\n",
    "\n",
    "    while not done:\n",
    "        epsilon = max(epsilon - change, e_end)\n",
    "        # env.render()\n",
    "        if frame % 10000 == 0:\n",
    "            DQN_target.setWeights(DQN.getWeights())\n",
    "\n",
    "        randaction_p = random.uniform(0, 1)\n",
    "\n",
    "        if randaction_p < epsilon or randPolicy:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(DQN.predict(state))\n",
    "\n",
    "        state1, reward, done, info = env.step(action)\n",
    "        totalReward += reward\n",
    "\n",
    "        replay_memory.append(Memory(state, action, reward, done, state1))\n",
    "\n",
    "        if not randPolicy and learnStep % 4 is 0:\n",
    "            batch = random.sample(replay_memory, min(32, len(replay_memory)))\n",
    "            X, Y = processBatch(batch, df, DQN, DQN_target)\n",
    "            DQN.fit(X, Y)\n",
    "            #plotting\n",
    "            # if frame % 500 == 0:\n",
    "            #     plot_rewards(rewards)\n",
    "        learnStep +=1\n",
    "        state = state1\n",
    "        # env.render()\n",
    "        frame += 1\n",
    "\n",
    "    last100_ep_rewards.append(totalReward)\n",
    "    if ep % 100 is 0:\n",
    "        print(\"Episodes: {}, frames: {}, last100 rewards mean: {}, epsilon: {}\" .format(ep, frame, sum(last100_ep_rewards)/len(last100_ep_rewards), epsilon) )\n",
    "        DQN.saveModel()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
